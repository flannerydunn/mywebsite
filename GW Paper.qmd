---
title: "Paper Written at GW"
---

Here is a paper I wrote about Section 230 in my Media Organizations and Audiences Course at GW this spring! The paper itself describes the legal origins of Section 230, its effects on social media sites with a specific case study on Facebook, and an argument to reform the law in order to address the harm it has allowed.

# **The Case Against Section 230** 

##### *2 May 2023*

##### ***By: Flannery Dunn***

### *History of Section 230*

Section 230 was born in 1996 of the Communications Decency Act, stating that "No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider." In layman\'s terms, Section 230 prevents online intermediaries from being held liable for what individuals choose to post on the internet. 

Jeff Kosseff, author of *The Twenty-Six Words That Created the Internet,* cites two specific, unrelated cases in 1956 that would eventually reach the Supreme Court and lay the groundwork precedents for Section 230\'s passage. Farmers Educational & Cooperative Union of America v. WDAY, Inc. was a case concerning the broadcasting of a senate campaign speech in North Dakota in 1956. Arthur C. Townley, an Independent candidate, had his incendiary speech aired by a local television and radio broadcaster, WDAY. Due to the speech\'s inflammatory nature, Farmers Educational & Cooperative Union of America sued both Townley and WDAY for defamation. It was argued that the radio station was equally liable for broadcasting the speech, however, the claim was dismissed due to the current federal communications law, which mandated that each political opponent be given the opportunity to broadcast their message (​​Section 315 of the Federal Communications Act of 1934). Still, the union appealed, and the Supreme Court reviewed the North Dakota court\'s opinion in 1959, concluding that WDAY was protected due to the \"federal statute prohibiting any censorship of political speeches that were required under this federal equal time rule and that the statute therefore immunized stations from any lawsuits stemming from those speeches\" (Kosseff 16).

The second case, Smith v. California, involved a bookstore owner in Los Angeles that was arrested for violating Section 21.01.1 of the Los Angeles Municipal Code:

It shall be unlawful for any person to have in his possession any obscene or indecent writing, book, pamphlet, picture, photograph, drawing . . . in any place of business where ice-cream, soft drinks, candy, food, school supplies, magazines, books, pamphlets, papers, pictures or postcards are sold or kept for sale.

Eleazar Smith, the bookstore owner, had sold a book which was graphic (for 1950s standards). The country judge charged Smith for violating the obscenity ordinance, and he spent thirty days in jail before the Supreme Court reviewed the ruling. Smith\'s lawyer argued \"that the ordinance was unconstitutional because it applied even if Smith lacked any intent, knowledge of illegality, or other culpable state of mind\" (Kosseff 23).

Two months later, the Supreme Court reversed the California ruling. The Court acknowledged restriction of obscene material under the First Amendment and even validated its constitutionality. Still, the Court held that booksellers cannot be found liable for the books that they sell. Even more important, the Court was worried that certain obscenity laws would \"cause bookstores to self-censor non-obscene speech\" which raised \"the same First Amendment concerns as laws that directly prohibit legal speech\" (Kosseff 25).

While it is important to acknowledge that both Smith v. California and Farmers Educational & Cooperative Union of America v. WDAY, Inc. were both legally disputed under free speech, it may be even more crucial to understand how the outcomes of each case were ultimately decided based upon intermediary rights where \"companies would receive some protection from liability for \[distribution\]\" (Kosseff 11). However, a paradox within these First Amendment rulings formed: distributors were more liable if they knew more. That is to say, if distributors did not review the content of their books, newspapers, or online user comments, their First Amendment protections were heightened significantly more than if a distributor were to review that content. While these cases dealt with and established the constitutional protections of bookstores and broadcasters, it ultimately laid the basis for all distributors of information, such as websites and ISPs, important context to the ultimate passage of Section 230.

This groundwork was tested in the mid-1990s when two cases in New York state found themselves split on the issues of whether computer service providers were to be treated as publishers or distributors of their users' posts. Specifically, the two cases, Cubby, Inc. v. CompuServe, Inc. and Stratton Oakmont, Inc. v. Prodigy Services Co., were concerning defamation cases. The court in Cubby v. CompuServe held that an internet platform was not liable as a distributor, while Stratton Oakmont v. Prodigy Services held that websites carried out a publishing role for their users and therefore liability was assigned for what their users post. Ultimately, the overwhelming negative reaction to the Stratton Oakmont v. Prodigy case resulted in a solution from Congress. This solution would address the \"\[complaints\] of heightened costs to review many thousands of users\' posts online, as well as significant liability risks from their failure to adequately do so\" (Ulrich).

On February 8, 1996, President Bill Clinton signed the Telecommunications Act into law. It was thirteen years since the creation of the internet, but eight years before Facebook, nine years before YouTube, and 11 years before the technology that started with the iPhone\" ([Berens](https://www.digitalcenter.org/leadership/)). So, what is Section 230? At its core, Section 230 exhibits the idea of \"internet exceptionalism.\" As in, the creation of the internet was entirely and completely different from all media that came before it (Kosseff 78). The law itself allows internet providers and more modernly, social media websites, specific protections. It allows for engaged users to maintain freedom of expression without placing liability on the internet intermediaries in which people post on. In some ways, Section 230 protects free speech. Without the law, social media websites may be more inclined to filter or censor speech out of fear of legal repercussions.

In the internet\'s narrow scope of power at the time, Section 230 was molded and shaped as such. \"\[Lawmakers\] read it as broadly as possible, making Section 230 the vehicle for Internet exceptionalism in the United States\" (Kosseff 78). Therefore, Section 230 is applied generally, with the exception of federal criminal law violations and the creation of illegal/harmful content (although the definition of this is generally case-by-case). Section 230 does not just apply to sites like Facebook or Twitter, it includes any platform that would cater to or support the publication of media by users (think, third party content). That being said, these third-party posters are legally the publisher of content when users post. The liability is on the specific account user. Meaning, Section 230 also allows for the discretionary removal of content by online platforms, say, if a post violates the platform\'s guidelines or rules. Furthermore, \"Negligence and illegal behavior are some of the areas where Section 230 won\'t protect an online company\" (The Hartford). Despite this, the negligence defined in Section 230 is a damagingly broad term, and concepts like the neutral tool\'s doctrine (which states that websites whose features that may be used improperly still maintain Section 230 protection due to user misuse) often ensure broad protections against websites that cause more harm than good.

### *\"The Banality of Evil\"*

One of the most infamous cases of platform negligence is described in-depth in Max Fisher\'s *The Chaos Machine: The Inside Story of How Social Media Rewired Our Minds and Our World.* In 2018, the United Nations formally accused the company formerly known as Facebook of allowing the platform to elicit the Myanmar genocide. Fisher describes a Silicon Valley contractor at an outsourcing agency that reviewed user content at Facebook named Jacob, \"who had raised every alarm he could \[about Facebook\]\" (Fisher 4). Jacob approached Fisher in 2018 knowing that something was wrong but knew that systemically he could do nothing about it.

Upon being invited to Facebook\'s headquarters, Fisher had a stunning realization: \"Many at the company seemed almost unaware that the platform\'s algorithms and design deliberately shape users\' experiences and incentives, and therefore the users themselves\" (Fisher 7). It was clear that Facebook as a company truly believed that the app was a force for good above anything else. However, this could not be further from the truth. In a 2018 presentation to Facebook, appointed researchers of its technology warned company executives that the algorithm \"exploits the human brain\'s attraction to divisiveness\" (Fisher 9). Moreover, the algorithm was even found to be designed specifically to deliver divisive content to ensure that users stayed engaged on the app. Despite sharing this information with the company, Facebook ultimately \"shelved the research and largely rejected its recommendations, which called for tweaking the promotional systems that choose what users see in ways that might have reduced their time online\" (Fisher 9). Even when data was presented plainly, conducted by their own commissioned researchers, Facebook chose to overlook the dangers of their product. Additionally, in a self-commissioned audit of Facebook during the summer of 2020, it was revealed that Facebook algorithms \"\[drive\] people toward self-reinforcing echo chambers of extremism, training them to hate\" (Fisher 10). This could not be any clearer in the case of Facebook\'s involvement in the Myanmar Genocide.

In 2012, the United States promised to modernize Myanmar with the internet. Silicon Valley, of course, led the charge with big tech arrival. Moving online \"almost spontaneously\" was endorsed by the country itself. The Myanmar government stated: \"a person without a Facebook identity is like a person without a home address\" (Fisher 36). Therefore, when the rate of internet usage spiked from .5 percent to 40 percent, it was safe to say that Facebook was critical to the integration of the internet in Myanmar. The integration of Facebook was so successful and ran so deep, most people in Myanmar exclusively used the app to message, browse, and access news - to the point that individuals were not aware that any other means of communication or sources of news even existed.

While Facebook considered Myanmar a total success story, other actors were utilizing the newfound power of the social media platform. More specifically, an extremist Buddhist monk named Wirathu was utilizing Facebook\'s platform to spread messages about Myanmar\'s Rohingya Muslim population, and violent accusations of the religious minority were being popularly received as fact. Facebook was warned by a Stanford researcher in 2013 about Wirathu\'s hate speech, and the potential dangers of misinformation in a wholly unsupervised platform (only one moderator was employed to review posts in Burmese, the language spoken in Myanmar). However, the warnings were repeatedly dismissed by Facebook and the company went forward with its expansion due to the success of the integration. Wirathu\'s dangerous misinformation heightened when riots broke out as a result of some of his postings, in which he urged people and the government to invade Muslim citizens\' homes. The violence incited ultimately resulted in temporarily blocking Facebook in the country.

In his commentary on the *Chaos Machine*, Brad Berens of the Center for the Digital Future recalls the phrase \"the banality of evil.\" This phrase was first conceived by Hannah Arendt in her book about Adolph Eichmann, a chief Nazi official. The banality of evil is the idea that everyday people are capable of committing inhumane acts without the willingness or knowledge of the acts even happening. One would assert that Facebook, although complicit in the violence of Myanmar, simply fell victim to the banality of evil. Facebook was more concerned with why the platform was blocked and how it impacted the platform instead of intervening. It is eminently clear that Facebook knew it had power. Facebook knew the role it played in disseminating fabricated lies and misinformation in Myanmar. It knew that its algorithm polarized its users and incited violence, because their own research told Facebook so. But the needs of running a business and turning a profit comes first. And \"Facebook knew...That\'s not 20/20 hindsight. The scale of this problem was significant, and it was already apparent,\" according to David Madden, who ran a tech-startup accelerator in Myanmar. Yet Facebook took no accountability, and saw no legal repercussions, due in part to the limited oversight of Section 230 and the lack of legal grounds that would hold the company liable.

In its genesis, Section 230 served as a means to facilitate the growth of the internet and moderate user-generated content without the fear of legal repercussions. Despite major technological changes to the internet, Section 230 has maintained its broad scope and has become a shield to companies that otherwise would be held accountable for the harm they cause. There is a critical necessity for safeguards that will prevent future events online that incite real, physical danger in real life. The Myanmar Genocide was not the first implication of Facebook\'s negligence, and it was not the last.

### *Reality Equals Reform*

The reliance on users to govern their own speech on Facebook and other social media platforms lacks corporate responsibility that the industry must implement in order to address the dangers of the Internet\'s Wild West landscape. Benjamin Cramer argues that Section 230 harms even further by \"offering no incentives for discussions of ethics and accountability, which in turn incentivizes piecemeal solutions to momentary crises rather than deeper examinations of high-level and long-term problems\" (Cramer 136). If companies like Facebook are motivated externally to operate ethically, there would be a greater push for corporate responsibility. Instead, Section 230 absolves responsibility and accountability, and perpetuates a moral hazard due to the fact that there are no consequences.

Drawing upon the Corporate Social Responsibility ethical framework, coined by Milton Friedman, suggests that companies such as Facebook can implement ethical policies without dependence on legislation that affects Section 230. Engagement in Corporate Social Responsibility theory signifies \"a business that wishes to be perceived as ethical should become a responsible citizen of the communities in which it operates, even if this reduces profits\" (Cramer 137). While this theory does not align with Facebook\'s current policies, pressure from its stakeholders and stockholders may shift internal focus to reflect the values of Corporate Social Responsibility, ultimately making Facebook appear more trustworthy and even adding value to the company where it may have been monetarily lost. Cramer notes that implementation of Corporate Social Responsibility is not common in social media companies, but \"has been praised as a viable technique for a company to display good corporate citizenship\" (Cramer 138). Facebook may be able to implement previously unconsidered suggestions for solutions if they embrace some type of accountability (an ethical resolution instead of legal) as opposed to merely making up justifications to escape culpability for its actions.

Nonetheless, if Facebook and other social media websites would rather ignore their ethical atrocities and continue to fixate on profiting off their current design, legislative reform of Section 230 is the next viable option to address the safety hazards of social media platforms. Changing the law would enforce legal safeguards that incentivize companies to act ethically and also hold those who break the law accountable for their actions.

As late as February 2023, legislation was introduced in the House and Senate aiming to \"update a law that was meant to encourage service providers to develop tools and policies to support effective moderation and allows them to finally be held accountable for the harmful, often criminal behavior that exists on their platforms\" (SAFE TECH Act of 2023). The act would draw upon precedent and require social media sites to moderate dangerous content. In the past, \"Congress has acted \[\] to ensure that social media companies don\'t get blanket immunity after hosting information on their websites aimed at facilitating human or sex trafficking\" and the law would act similarly (SAFE TECH Act of 2023).

Other suggestions to amend Section 230 include revising the section to state:

No provider or user of an interactive computer service that takes reasonable steps to address known unlawful uses of its services that create serious harm to others shall be treated as the publisher or speaker of any information provided by another information content provider in any action arising out of the publication of content provided by that information content provider (\"It\'s Time to Update Section 230\").

This revision would effectively be able to hold providers accountable for their roles in supporting harm, while also ensuring protections when they have taken reasonable steps to protect their users.

Regardless of the route taken to make online spaces safer, and in turn the real world, the failures of Section 230 must be corrected. Whether by ethical motivations or legal, there is an existing duty to ensure online safety, it simply depends on what will be the external factor to tip the scale towards accountability.

Works Cited

Cramer, Benjamin. *From Liability to Accountability: The Ethics of Citing Section 230 to Avoid the Obligations of Running a Social Media Platform.* Journal of Information Policy 1 May 2020; 10 123--150. doi: <https://doi.org/10.5325/jinfopoli.10.2020.0123>

Fisher, Max. *The Chaos Machine: The Inside Story of How Social Media Rewired Our Minds and Our World.* Little, Brown and Company. Kindle Edition. September 2022.

Kosseff, Jeff. *The Twenty-Six Words That Created the Internet*. Cornell University Press, 2019. *JSTOR*, http://www.jstor.org/stable/10.7591/j.ctvr7fcrd. Accessed 29 Apr. 2023.

Mark R. Warner. \"Legislation to Reform Section 230 Reintroduced in the Senate, House.\" Feb 28, 2023. <https://www.warner.senate.gov/public/index.cfm/2023/2/legislation-to-reform-section-230-reintroduced-in-the-senate-house>.  Press release.

Ortutay, Barbara. \"What you should know about Section 230, the rule that shaped today\'s internet.\" *PBS.* February 21, 2023. <https://www.pbs.org/newshour/politics/what-you-should-know-about-section-230-the-rule-that-shaped-todays-internet>.

Ulrich, Ashley. \"The Internet\'s Wild West: Section 230 and Why Platforms Don\'t Owe You Anything.\" *The Blog.* May 20, 2020. <https://blog.jipel.law.nyu.edu/2020/05/the-internets-wild-west-section-230-and-why-platforms-dont-owe-you-anything/>.

Reuters. \"Myanmar: UN blames Facebook for spreading hatred of Rohingya.\" *The Guardian.* March 12, 2018.
